{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "> fastai and Huggingface transformers\n",
    "\n",
    "Documentation of the core functionalities using sequence classification as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core\n",
    "# export\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "from fastai.interpret import ClassificationInterpretation\n",
    "from fastai.text.all import *\n",
    "from fastai.data.all import TransformBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n",
    "model = AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TransformersTokenizer(Transform):\n",
    "    \"\"\"A wrapper for the tokenizer from Huggingface transformers\n",
    "    Arguments: \n",
    "        tokenizer: the tokenizer class from Hugginface transformers\n",
    "        seq_len: the max length ot the sequences\n",
    "        truncation: if the sequences exceeding seq_len should be truncated \n",
    "                    (should be true if seq_len is provided)\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, seq_len, truncation): \n",
    "        # not all tokenizers require seq_len and truncation as arguments\n",
    "        # and will throw a warning if they do. The problem is, that those \n",
    "        # tokenizers also seem to get stuck in an endless  loop if they get \n",
    "        # passed these argumtens. I tried a lot, but I found no sophisticated \n",
    "        # way to catch the warning. However, those seem to already return  \n",
    "        # ids (converted to str) instead of str tokens, so this way one might\n",
    "        # recognize them\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenize = self.tokenizer.tokenize\n",
    "        x = self.tokenize('Tokenize a text, to see if it already returns ids instead of tokens')\n",
    "        \n",
    "        if not self.is_int_as_str(x):     \n",
    "            self.tokenize = partial(self.tokenizer.tokenize, \n",
    "                                max_length=seq_len, \n",
    "                                truncation=truncation)\n",
    "            \n",
    "    def encodes(self, x:str):\n",
    "        toks = self.tokenize(x)\n",
    "        ids = tensor(self.tokenizer.convert_tokens_to_ids(toks)) \n",
    "        return TensorText(ids)\n",
    "    \n",
    "    def decodes(self, x:TensorText): \n",
    "        return TitledStr(self.tokenizer.decode(x.cpu().numpy()))\n",
    "    \n",
    "    def is_int_as_str(self, x):\n",
    "        try: \n",
    "            int(x[0])\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TextBlockWithCustomTokenizer(TransformBlock):\n",
    "    \"a TransformBlock with custom Tokenizer from Hugginface transformers\"\n",
    "    def __init__(self, tok, is_lm, seq_len, backwards=False, **kwargs):\n",
    "        truncation = True if seq_len else None\n",
    "        type_tfms = [TransformersTokenizer(tok, seq_len, truncation, **kwargs)]\n",
    "        if backwards: type_tfms += [reverse_text]\n",
    "        return super().__init__(type_tfms=type_tfms,\n",
    "                                dl_type=LMDataLoader if is_lm else SortedDL,\n",
    "                                dls_kwargs={'seq_len': seq_len} if is_lm else {'before_batch': Pad_Chunk(seq_len=seq_len)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@patch(cls_method=True)\n",
    "@delegates(DataLoaders.from_dblock)\n",
    "def from_df_with_custom_tok(cls:TextDataLoaders, df, path='.', valid_pct=0.2, seed=None, text_col=0, label_col=1, \n",
    "                            label_delim=None, y_block=None, is_lm=False, valid_col=None, custom_tok=None, seq_len=72, \n",
    "                            backwards=False, **kwargs):\n",
    "    \"Create from `df` in `path` with `valid_pct` and custom tokenizer\"\n",
    "    blocks = [TextBlockWithCustomTokenizer(custom_tok, is_lm, seq_len, backwards)]\n",
    "    if y_block is None and not is_lm:\n",
    "        blocks.append(MultiCategoryBlock if is_listy(label_col) and len(label_col) > 1 else CategoryBlock)\n",
    "    if y_block is not None and not is_lm: blocks += (y_block if is_listy(y_block) else [y_block])\n",
    "    splitter = RandomSplitter(valid_pct, seed=seed) if valid_col is None else ColSplitter(valid_col)\n",
    "    dblock = DataBlock(blocks=blocks,\n",
    "                       get_x=ColReader(\"text\"),\n",
    "                       get_y=None if is_lm else ColReader(label_col, label_delim=label_delim),\n",
    "                       splitter=splitter)\n",
    "    return cls.from_dblock(dblock, df, path=path, seq_len=seq_len, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TextDataLoaders.from_df_with_custom_tok(df, custom_tok = tokenizer, bs = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes, i was lucky enough to see the long - running original production of michael bennett's hit musical. it was an amazing experience and i paid to see the movie when it hit theatres back in 1985. it is awful. almost everything fails. first off, attenborough ( a fine actor, a good director with the right material ) is a sorry</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i thought the film could be a bit more complex, in a psychological sense perhaps, but the action and voice acting were top notch. the animation was heavy cg in many scenes, but very good ones at that. this is one of the batman returns / forever type films, which include romances and the conflicts of wayne and motives for dating. 007</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DropOutput(Callback):\n",
    "    \"Drops some of the output form the transformes model, which is not needed for fastai\"\n",
    "    def after_pred(self): self.learn.pred = self.pred['logits']      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_transformer_classification_model(model, n_out, model_name):\n",
    "    \"creates a classification model from hugginface transformers\"\n",
    "    if model_name:\n",
    "            # try loading the model wiht custom number of classes, fails for some pretrained models\n",
    "        try: \n",
    "            model = model.from_pretrained(model_name, num_labels = n_out)\n",
    "        # if above fails, force a new classifiert onto the model\n",
    "        except: \n",
    "            model = model.from_pretrained(model_name)\n",
    "            last_layer = model.classifier\n",
    "            if not last_layer.out_features == n_out: \n",
    "                warn('Randomly initializing the last layer of the model, as the requested number of classes '\n",
    "                     'did not match the number of classes specified in the models config file. '\n",
    "                     'You should probably TRAIN this model on a down-stream task to be able to use it for '\n",
    "                     'predictions and inference.')\n",
    "                model.classifier = nn.Linear(last_layer.in_features, n_out)\n",
    "    else: \n",
    "        config = model.config_class()\n",
    "        config.num_labels = n_out\n",
    "        model = model(config = config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@delegates(Learner.__init__)\n",
    "def transformers_classifier_learner(dls, arch, seq_len=72,backwards=False, model_name=None, \n",
    "                                    n_out=None, y_range=None, cbs=None, loss_func=None, **kwargs):\n",
    "    \"creates a Learner class for classification using Huggingface transformers\"\n",
    "    if n_out is None: n_out = get_c(dls)\n",
    "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
    "    model = create_transformer_classification_model(model=arch, n_out=n_out, model_name=model_name)\n",
    "    assert seq_len <= model.config.max_position_embeddings, 'seq_len exceeds the max numbers of embeddings for this model'\n",
    "    if not isinstance(cbs, list): cbs = [] if cbs is None else [cbs]\n",
    "    cbs.append(DropOutput())\n",
    "    learn = Learner(dls, model, loss_func = CrossEntropyLossFlat() if loss_func is None else loss_func, cbs=cbs, **kwargs)\n",
    "    if model_name: learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastai.interpret import ClassificationInterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "@patch\n",
    "def print_multilabel_classification_report(self:ClassificationInterpretation):\n",
    "    \"Calculates common classification metrics for each label separately.\"\n",
    "    def accuracy(x, y): return (x == y).float().mean().numpy()\n",
    "    target_name = [str(v) for v in self.vocab]\n",
    "    metrics = [accuracy, RocAuc(), F1Score(), Recall(), Precision(), Jaccard(), MatthewsCorrCoef()]\n",
    "    res = pd.DataFrame({'metrics': ['accuracy', *[x.name for x in metrics[1:]]]})\n",
    "    for d,t,n in zip(torch.unbind(self.decoded,1), torch.unbind(self.targs, 1), target_name):\n",
    "        res[n] = [metric(d if metric=='roc_auc_score' else d.sigmoid().round(),t) for metric in metrics]\n",
    "    res_t = res.T\n",
    "    res_t.columns = list(res_t.iloc[0, :])\n",
    "    return res_t[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# cuda\n",
    "learn = transformers_classifier_learner(dls, model, \n",
    "                                        model_name=pretrained_weights, \n",
    "                                        metrics = accuracy, \n",
    "                                        cbs = [SaveModelCallback(), EarlyStoppingCallback(patience=5)]).to_fp16()\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.688082</td>\n",
       "      <td>0.656907</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.699804</td>\n",
       "      <td>0.764321</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703906</td>\n",
       "      <td>0.691027</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with valid_loss value: 0.6569069623947144.\n"
     ]
    }
   ],
   "source": [
    "# cuda\n",
    "learn.fit_one_cycle(3, 1e-4, wd = 1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:faimed3d]",
   "language": "python",
   "name": "conda-env-faimed3d-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
